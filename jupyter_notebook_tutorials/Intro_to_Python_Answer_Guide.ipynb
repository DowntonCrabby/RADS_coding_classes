{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo practice problem 1 answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Solve and save the result of 23 time 71 as a variable\n",
    "var_1 = 23*71\n",
    "# Now solve and save the result of 6 to the power of 3 as another, differently named variable\n",
    "var_2 = 6 ** 3\n",
    "# Finally, check if the second result is greater than the first result and print the final answer!!\n",
    "comp = var_2 > var_1\n",
    "print(comp)\n",
    "#alternative (equally correct) final answer: \n",
    "print(var_2 > var_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of presynaptic sites on this neuron\n",
    "num_presyn = 79 \n",
    "\n",
    "# number of postsynaptic sites on this neuron\n",
    "num_postsyn = 805 \n",
    "\n",
    "# angle of the longest branch \n",
    "apical_angle = 7.928 \n",
    "\n",
    "#lengths of each branch in our neuron\n",
    "branch_lengths = [ 7, 39,  2, 48, 40, 38,  6, 16, 18, 49, 32, 10, 52, 22,  1, 13, 16,\n",
    "        46, 18, 15,  3, 32, 28, 27, 49, 37, 16, 49, 38, 48,  8,  3, 10,  4,\n",
    "        3,  9, 45,  9,  41,  9,  2,  1,  3, 21,  5, 24, 35, 47, 13, 18, 16,\n",
    "       11,  43, 24, 10, 41, 32, 43, 33, 24, 25, 34, 38, 53, 28, 32,  41,  5,\n",
    "        49, 38, 32,  18, 33,  7, 27, 26, 40, 26, 33, 16, 41,  4, 14,  35, 25,\n",
    "       22, 39, 14, 20, 21, 10,  5, 43, 42, 25, 48, 38, 32, 18, 41] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884\n"
     ]
    }
   ],
   "source": [
    "num_synapses = num_presyn + num_postsyn\n",
    "# you could also use sum, but you would have to pass a list of your pre and post numbers:\n",
    "num_synapses = sum([num_presyn, num_postsyn])\n",
    "print(num_synapses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "longest_branch = max(branch_lengths)\n",
    "print(longest_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "num_branches = len(branch_lengths)\n",
    "print(num_branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.4\n"
     ]
    }
   ],
   "source": [
    "average_branch_length = sum(branch_lengths)/num_branches\n",
    "print(average_branch_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo practice problem 2 answers\n",
    "There are many ways to solve this problem, so if you went about it differently but got the right answers, then full points to you! I've solved it here using only tools we learned in this class, but there are many ways to make this answer far more powerful, more quick, and more easy to read. If you just want to see the answers, and not the ways in which I solved for them, scroll directly to the end! If you want an in-depth explanation for how I solved this problem using only the tools learned in class these last two days, read on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_1_data = [1.1, 9118342.7791, 'P81.1.1', 'r01', 7.348, 1,\n",
    "              1.2, 9118342.7792, 'P81.1.1', 'r01', 7.348, 1,\n",
    "              1.3, 9118342.7793, 'P81.1.1', 'r02', 7.348, 2,\n",
    "              1.4, 9118342.7794, 'P81.1.1', 'r01', 7.348, 1,\n",
    "              1.5, 9118342.7795, 'P81.1.1', 'r01', 7.348, 1]\n",
    "exp_2_data = [2.1, 9118342.7796, 'P81.1.1', 'r01', 7.348, 1,\n",
    "              2.2, 9118342.7797, 'P81.1.1', 'r02', 7.348, 2,\n",
    "              2.3, 9118342.7798, 'P81.1.1', 'r02', 7.348, 2,\n",
    "              2.4, 9118342.7799, 'P81.1.1', 'r02', 7.348, 1,\n",
    "              2.5, 9118342.7800, 'P81.2.1', 'r02', 8.671, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My answer here may seem exploratory, and that's because it is! In real problems that Emily and I tackle at work, there's often a large amount of data exploration that happens before the answer can be found. By breaking this up into smaller problems, you're able to organize your thoughts and carefully compare within and across more complex experiments (and larger groups of experiments) without getting confused or losing any potentially critical information.\n",
    "\n",
    "Notice how I've split the data itself up, so I can get a good sense of it, rather than leaving it in one long string. This doesn't change how Python sees the data (as far as it can tell, the spacing between the original experiment lists and mine is the same), but it helps me get a good sense of what I'm looking at. I can see some inconsistencies already, so let's write some code that proves they're there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(exp_1_data) == len(exp_2_data)) #determine if there is any data missing wholesale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there aren't any whole missing values, I'm going to explore each experiment individually, as the inconsistencies seem to be internal to the experiments from a quick glance at the data. I won't be looking at the unique IDs, as I can see that it's good for them to be different from one another. If you're feeling adventurous, you can write a function to check each of the unique IDs to make sure none are the same, which is normally good practice, but you'll start to notice how my comparison functions start to look very similar to one another after a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 1 result: \n",
      "no problems found\n",
      "experiment 2 result: \n",
      "no problems found\n"
     ]
    }
   ],
   "source": [
    "def compare_batch_numbers(expt_data):\n",
    "    problem_item = 0.0\n",
    "    comparison_1 = 0.0 \n",
    "    #initialize a comparison value here, so you can store something else here later\n",
    "    #Everything done inside the for loop gets \"forgotten\" by the program once the loop starts again\n",
    "    #so be sure to save anything you want to work on between loops outside. If you want to learn more\n",
    "    #about how to solve this, read up on python scope!\n",
    "    for item in expt_data:\n",
    "        if type(item) == float: #use the reserved word here, since that's what the type will return\n",
    "            if item > 3.0: \n",
    "            #batch numbers are all in the millions, and should be off only by .0004 from one another at most.\n",
    "            #let's check that that's true!\n",
    "                if comparison_1 == 0.0: #if this is zero, nothing has been set yet, and we've only done the loop once\n",
    "                    comparison_1 = item #set the first comparison value to the first batch number in the list\n",
    "                else: #if you've already set the comparison value\n",
    "                    if item - comparison_1 > 0.0004:\n",
    "                        problem_item = item #if this value is too large, then you've found a problem! Save that information!\n",
    "                        print(item)\n",
    "    if problem_item != 0.0: #if you found a problem item, return it\n",
    "        return problem_item\n",
    "    else:\n",
    "        return \"no problems found\"\n",
    "    \n",
    "#now lets call the function and see if there are any differences lurking here!\n",
    "print('experiment 1 result: ') #some fluff, to make reading results easier\n",
    "print(compare_batch_numbers(exp_1_data))\n",
    "print('experiment 2 result: ')\n",
    "print(compare_batch_numbers(exp_2_data)) #Can you see how functions are useful here? We just saved a lot of time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we've proven that there are no problems in the batch numbers. We could look next at the process IDs in the same way, but as the description of the experimental values explains, there should be a) identical process IDs for each sub_experiments, and b) an identical experimental value for each process ID. So ideally, we'd have 10 identical process IDs, and 10 identical batch ids. Lets see how we'd check for the validity of both of those facts in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 1 result: \n",
      "no problems found\n",
      "experiment 2 result: \n",
      "P81.2.1\n",
      "8.671\n",
      "('P81.2.1', 8.671)\n"
     ]
    }
   ],
   "source": [
    "def compare_process_and_ev(exp_data):\n",
    "    #storing variables out here again, but this time we'll store one for process and another for experimental value\n",
    "    proc_id = '' #these are best kept as strings for now. \n",
    "    #You can set this as an empty string, or as a string of your choice. I tend to prefer empty strings, as then\n",
    "    #you can be totally sure that that string doesn't show up anywhere else in your data. If you accidentally choose\n",
    "    #a string that's in use elsewhere in your data, your script (and as a result, you) would get super confused!\n",
    "    exp_val = 0.0\n",
    "    problem_id = ''\n",
    "    problem_val = 0.0\n",
    "    for item in exp_data:\n",
    "        if type(item) == str: #again, use the reserved word 'str' here\n",
    "            if len(item) > 3: #don't want to store rig IDs\n",
    "                if len(proc_id) > 0: #if there's already a set process_id, do the following\n",
    "                    if item != proc_id: #if the process ID and current process ID differ\n",
    "                        problem_id = item #store the problem ID\n",
    "                        print(problem_id) #and print it, just for fun\n",
    "                else: #if there isn't already a set process ID, do the following\n",
    "                    proc_id = item #set the proc ID to the current item.\n",
    "        if type(item) == float: #have to be careful to not look at unique ID or batch number here!\n",
    "            if 3.0 < item < 100.0: #if it's larger than a unique id but smaller than a batch number, continue\n",
    "                if exp_val > 0.0: #if you've already set the experiment_value (same logic as above for process IDs)\n",
    "                    if item != exp_val:\n",
    "                        problem_val = item\n",
    "                        print(problem_val)\n",
    "                else:\n",
    "                    exp_val = item\n",
    "    if problem_id == '':\n",
    "        if problem_val == 0.0:\n",
    "            return 'no problems found'\n",
    "    else:\n",
    "        return problem_id, problem_val #see how you can return two things?\n",
    "    \n",
    "print('experiment 1 result: ')\n",
    "print(compare_process_and_ev(exp_1_data))\n",
    "print('experiment 2 result: ')\n",
    "print(compare_process_and_ev(exp_2_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've found an issue with experiment 2 already, and are done with exploring 4 out of the 5 experimental value categories already! We're not going to explore the results, as they're not the causes of anything, and we already know that there are differences there. Let's take a look at our last category, the Rig IDs, and see if we catch any other issues in the experimental procedures. You're probably getting the hang of things at this point, so feel free to open up a fresh notebook and try to solve this portion of the problem without looking at the answer first, if you havent already, for practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 1 result: \n",
      "s01\n",
      "expeirment 2 result: \n",
      "s02\n"
     ]
    }
   ],
   "source": [
    "def compare_rigs(exp_data):\n",
    "    rig_id = '' #here, we know what the rig ID should be.  But how do we know what experiment we're looking at?\n",
    "    #we could add a parameter that clarifies, or we could do the following (knowing what we know about the unique\n",
    "    #id by which the experiment is identified)\n",
    "    problem_id = ''\n",
    "    for item in exp_data:\n",
    "        if type(item) == float:\n",
    "            if item < 3: #if it's the unique ID\n",
    "                if item < 2:\n",
    "                    rig_id = 'r01'\n",
    "                else:\n",
    "                    rig_id = 'ro2'\n",
    "        if type(item) == str:\n",
    "            if len(item) < 5: #we don't want to capture any process IDs,\n",
    "                #but want to leave room for the possibility of a completely malformed rig ID\n",
    "                if item != rig_id: #note that, since we've set the rig id with another method earlier, \n",
    "                    #we dont have to set it again now\n",
    "                    problem_id = item \n",
    "    if len(problem_id) > 0: #switching it up for fun\n",
    "        return problem_id\n",
    "    else:\n",
    "        return 'no problems found'\n",
    "                \n",
    "print('experiment 1 result: ')\n",
    "print(compare_rigs(exp_1_data))\n",
    "print('experiment 2 result: ')\n",
    "print(compare_rigs(exp_2_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You've now solved the problem of the failed experiment, and your boss is very happy to see your answers. \n",
    "\n",
    "   1. You've shown that experiment 2 had problems with its batch ID/experimental parameters (the incorrect ones were ('P81.2.1', 8.671) - bonus points if you included the unique experiment subID for the incorrect values (1.3, if you didn't, try that now!)\n",
    "   2. You've shown that experiment 1 had an incorrect rig used for one subexperiment (s02) - bonus points again for including unique experiment sub ID. \n",
    "   3. And finally, you've shown that experiment 2 had another issue, also with an incorrect rig used - s01 (for the first sub experiment), when all should have been s02. \n",
    "\n",
    "In future classes where we explore using pandas, we find ways to make this process even faster and more efficient, for exploring datasets such as the ones provided by the Allen Institute SDK, and also find ways to solve these problems by connecting the data source to another type of common tool, such as excel or powerbi. Looking forward to seeing you all there, and hope you keep practicing and using the tools we learned here in the meantime!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good way to try to learn more about this subject yourself is to look for information on list comprehension, dictionaries, indexing lists, and string manipulation to try to come up with a more efficient answer. If you're feeling super adventurous, try solving this more efficiently using the tools mentioned above, and then look into the python timeit function (an advanced tool used to determine a function's average runtime). See if you can come up with a more efficient answer! Of course, for toy problems such as this one it isn't necessary to create efficiencies, but when working with petabytes of data it can become the difference between your program running for days vs hours. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
